import streamlit as st
from pathlib import Path
from ..core.prompts import MODEL_INFO, FAUST_QUICK_PROMPTS


def render_project_management(glm_system):
    """Render project management section with file handling"""
    # Add styling for better visibility
    st.markdown(
        """
    <style>
    .project-management {
        background-color: #f0f8ff;
        padding: 15px;
        border-radius: 10px;
        border: 2px solid #4CAF50;
        margin-bottom: 20px;
    }
    </style>
    """,
        unsafe_allow_html=True,
    )

    with st.container():
        st.markdown('<div class="project-management">', unsafe_allow_html=True)
        st.subheader("üìÅ Project Management")

        # Initialize current project if not set
        if "current_project" not in st.session_state:
            st.session_state.current_project = "Default"

        col1, col2, col3 = st.columns(3)

        with col1:
            available_projects = glm_system.project_manager.get_project_list()
            selected_project = st.selectbox(
                "üìÇ Current Project:",
                options=available_projects,
                index=(
                    available_projects.index(st.session_state.current_project)
                    if st.session_state.current_project in available_projects
                    else 0
                ),
                help="Organize your chats and work by project",
                key="project_selector",
            )

        with col2:
            new_project = st.text_input(
                "‚ûï New Project:", placeholder="e.g., MyAudioApp"
            )
            if new_project and st.button("Create Project", key="create_proj"):
                success, message = glm_system.project_manager.create_project(
                    new_project
                )
                if success:
                    st.success(message)
                    st.rerun()
                else:
                    st.error(message)

        with col3:
            if selected_project != "Default" and st.button(
                "üóëÔ∏è Delete", key="delete_proj"
            ):
                success, message = glm_system.project_manager.delete_project(
                    selected_project
                )
                if success:
                    st.success(message)
                    st.rerun()
                else:
                    st.error(message)

        # Handle project change
        if selected_project != st.session_state.current_project:
            handle_project_change(selected_project)

        st.session_state.current_project = selected_project

        st.markdown("</div>", unsafe_allow_html=True)
        st.divider()
        return selected_project


def handle_project_change(new_project):
    """Handle project change with file management options"""
    # Check if there are open files
    open_files = st.session_state.get("editor_open_files", {})

    if open_files:
        # Check if any files have unsaved changes
        unsaved_files = []
        for file_path, file_data in open_files.items():
            if file_data.get("has_unsaved_changes") or file_data.get(
                "has_ai_suggestions"
            ):
                unsaved_files.append(Path(file_path).name)

        # Show file management dialog
        st.warning(f"üîÑ **Switching to project: {new_project}**")

        if unsaved_files:
            st.error(f"‚ö†Ô∏è **You have {len(unsaved_files)} files with unsaved changes:**")
            for filename in unsaved_files:
                st.write(f"‚Ä¢ {filename}")

        st.info(f"üìÅ **Currently open files:** {len(open_files)} files")

        # File management options
        st.subheader("ü§î What would you like to do with your open files?")

        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button(
                "üíæ **Save All & Close**", key="save_all_close", type="primary"
            ):
                save_all_and_close_files()
                st.success("‚úÖ All files saved and closed!")
                st.rerun()

        with col2:
            if st.button("üóô **Close Without Saving**", key="close_all_no_save"):
                if unsaved_files:
                    # Show confirmation for unsaved changes
                    if "confirm_close_all" not in st.session_state:
                        st.session_state.confirm_close_all = False

                    if not st.session_state.confirm_close_all:
                        st.error("‚ö†Ô∏è **This will discard all unsaved changes!**")
                        if st.button(
                            "‚ö†Ô∏è **Confirm: Close Without Saving**",
                            key="confirm_close_all_btn",
                        ):
                            st.session_state.confirm_close_all = True
                            close_all_files()
                            st.success("üóô All files closed without saving")
                            st.rerun()
                    else:
                        close_all_files()
                        st.success("üóô All files closed")
                        st.rerun()
                else:
                    close_all_files()
                    st.success("üóô All files closed")
                    st.rerun()

        with col3:
            if st.button("üìå **Keep Files Open**", key="keep_files_open"):
                st.info("üìå Files will remain open for reference across projects")
                # Just clear the confirmation state and continue
                if "confirm_close_all" in st.session_state:
                    del st.session_state.confirm_close_all
                st.rerun()

        # Add informational note
        st.caption(
            "üí° **Tip:** Keeping files open lets you reference code from other projects while working"
        )

        # Prevent further execution until user makes choice
        st.stop()


def save_all_and_close_files():
    """Save all open files and close them"""
    open_files = st.session_state.get("editor_open_files", {})

    for file_path, file_data in open_files.items():
        try:
            # Determine what content to save
            content_to_save = file_data.get("ai_suggested_content") or file_data.get(
                "current_content", ""
            )

            if content_to_save and (
                file_data.get("has_unsaved_changes")
                or file_data.get("has_ai_suggestions")
            ):
                # Save the file
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(content_to_save)
        except Exception as e:
            st.error(f"Error saving {Path(file_path).name}: {e}")

    # Clear all open files
    st.session_state.editor_open_files = {}

    # Clear confirmation states
    keys_to_clear = []
    for key in list(st.session_state.keys()):
        # Ensure key is a string before using startswith()
        if isinstance(key, str) and key.startswith("confirm_close"):
            keys_to_clear.append(key)

    for key in keys_to_clear:
        try:
            del st.session_state[key]
        except KeyError:
            pass  # Key might have been deleted already


def close_all_files():
    """Close all open files without saving"""
    # Clear all open files
    st.session_state.editor_open_files = {}

    # Clear all editor-related session state
    keys_to_clear = []
    for key in list(st.session_state.keys()):
        # Ensure key is a string before using startswith()
        if isinstance(key, str) and any(
            key.startswith(prefix)
            for prefix in ["editor_", "confirm_close", "ai_prompt_", "ai_model_"]
        ):
            keys_to_clear.append(key)

    for key in keys_to_clear:
        try:
            del st.session_state[key]
        except KeyError:
            pass  # Key might have been deleted already


# Keep all other functions the same...
def render_model_selection(glm_system):
    """Render hybrid model selection section with routing modes"""
    # Add styling for better visibility
    st.markdown(
        """
    <style>
    .model-selection {
        background-color: #fff8dc;
        padding: 15px;
        border-radius: 10px;
        border: 2px solid #FF9800;
        margin-bottom: 20px;
    }
    .routing-mode {
        background-color: #f0f8ff;
        padding: 10px;
        border-radius: 5px;
        border-left: 4px solid #2196F3;
        margin: 10px 0;
    }
    </style>
    """,
        unsafe_allow_html=True,
    )

    with st.container():
        st.markdown('<div class="model-selection">', unsafe_allow_html=True)
        
        # Routing Mode Selection
        st.markdown('<div class="routing-mode">', unsafe_allow_html=True)
        st.subheader("ü§ñ Model Selection & Routing")
        
        col1, col2 = st.columns(2)
        with col1:
            routing_mode = st.radio(
                "Routing Mode",
                ["üöÄ Auto (HRM Decides)", "üéØ Manual Selection", "üí° Assisted (See Recommendations)"],
                index=0,
                help="Choose how models are selected for your tasks"
            )
        
        with col2:
            use_context = st.checkbox(
                "üìö Use Knowledge Base",
                value=True,
                help="Include uploaded documents in responses",
            )
            
            # HRM Debug Mode
            hrm_wrapper = getattr(glm_system, 'hrm_wrapper', None)
            if hrm_wrapper:
                debug_hrm = st.checkbox(
                    "üîç HRM Debug Mode",
                    value=False,
                    help="Show HRM task decomposition details in responses",
                    key="hrm_debug_mode"
                )
            else:
                debug_hrm = False
        
        # Convert routing mode to simple string
        routing_mode_map = {
            "üöÄ Auto (HRM Decides)": "auto",
            "üéØ Manual Selection": "manual", 
            "üí° Assisted (See Recommendations)": "assisted"
        }
        routing_mode_key = routing_mode_map[routing_mode]
        
        # Model Selection based on routing mode
        if routing_mode == "üéØ Manual Selection":
            model_options = list(glm_system.models.keys())
            selected_model = st.selectbox(
                "Choose Model",
                options=model_options,
                help="GLM-Z1 for complex reasoning, Code Llama for FAUST/DSP, DeepSeek for optimization",
            )
            
            # Show model info
            if selected_model in MODEL_INFO:
                st.info(MODEL_INFO[selected_model])
                
        else:
            selected_model = "auto"
            if routing_mode == "üöÄ Auto (HRM Decides)":
                hrm_status = "üß† HRM will analyze your request and route to the optimal model automatically"
                if getattr(glm_system, 'hrm_wrapper', None):
                    device = getattr(glm_system.hrm_wrapper, 'device', 'unknown')
                    if device == 'mps':
                        hrm_status += " ‚ö° (M4 Max MPS acceleration)"
                    elif device == 'cuda':
                        hrm_status += " üöÄ (CUDA acceleration)"
                st.info(hrm_status)
            else:  # Assisted mode
                st.info("üí° HRM will analyze your request and show recommendations, but you can override")
        
        st.markdown('</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
        
        return selected_model, use_context, routing_mode_key, debug_hrm


def render_sidebar(glm_system):
    """Render sidebar with file management"""
    with st.sidebar:
        st.header("üìö Knowledge Base Management")

        # File upload section
        render_file_upload_section(glm_system)

        # Bulk operations
        render_bulk_operations(glm_system)

        # FAUST Documentation Section
        render_faust_docs_section(glm_system)

        # Model status
        render_model_status(glm_system)


def render_file_upload_section(glm_system):
    """Render file upload and organization section"""
    st.subheader("üìÇ File Upload & Organization")

    subfolder_options = [
        "üìÅ Root (uploads/)",
        "üéµ FAUST",
        "üíª C++",
        "üêç Python",
        "üéµ JUCE",
        "üîä DSP",
        "üìä General",
        "üñºÔ∏è Images",
        "üìù Documentation",
        "üîß Custom...",
    ]

    selected_subfolder = st.selectbox("Choose subfolder:", subfolder_options)

    # Handle custom subfolder
    target_subfolder = ""
    if selected_subfolder == "üîß Custom...":
        custom_folder = st.text_input(
            "Enter custom folder name:", placeholder="e.g., my_project"
        )
        if custom_folder:
            target_subfolder = custom_folder.strip()
    elif selected_subfolder != "üìÅ Root (uploads/)":
        folder_mapping = {
            "üéµ FAUST": "faust",
            "üíª C++": "cpp",
            "üêç Python": "python",
            "üéµ JUCE": "juce",
            "üîä DSP": "dsp",
            "üìä General": "general",
            "üñºÔ∏è Images": "images",
            "üìù Documentation": "docs",
        }
        target_subfolder = folder_mapping.get(selected_subfolder, "")

    # File upload
    uploaded_files = st.file_uploader(
        "Upload Files",
        accept_multiple_files=True,
        type=[
            "pdf",
            "txt",
            "md",
            "py",
            "cpp",
            "h",
            "c",
            "hpp",
            "cc",
            "dsp",
            "lib",
            "jpg",
            "png",
            "bmp",
            "tiff",
        ],
        help=(
            f"Files will be saved to: uploads/{target_subfolder}"
            if target_subfolder
            else "Files will be saved to: uploads/"
        ),
    )

    if uploaded_files:
        for uploaded_file in uploaded_files:
            if target_subfolder:
                upload_path = Path("./uploads") / target_subfolder / uploaded_file.name
            else:
                upload_path = Path("./uploads") / uploaded_file.name

            upload_path.parent.mkdir(parents=True, exist_ok=True)

            with open(upload_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            with st.spinner(f"Processing {uploaded_file.name}..."):
                result = glm_system.file_processor.process_file(str(upload_path))

            folder_display = f"{target_subfolder}/" if target_subfolder else "root/"
            st.success(f"‚úÖ Saved to {folder_display} - {result}")


def render_bulk_operations(glm_system):
    """Render bulk operations section"""
    st.subheader("üîÑ Bulk Operations")
    col1, col2 = st.columns(2)

    with col1:
        if st.button("üîç Scan All Subfolders"):
            with st.spinner("Scanning all subfolders..."):
                result = glm_system.file_processor.scan_uploads_recursive()
            st.success(result)

    with col2:
        if st.button("üìä Folder Stats"):
            stats = glm_system.file_processor.get_folder_stats()
            if stats:
                st.write("üìÇ **File counts by folder:**")
                for folder, count in stats.items():
                    emoji = {
                        "faust": "üéµ",
                        "cpp": "üíª",
                        "python": "üêç",
                        "juce": "üéµ",
                        "dsp": "üîä",
                        "general": "üìä",
                        "images": "üñºÔ∏è",
                        "docs": "üìù",
                        "root": "üìÅ",
                    }.get(folder, "üìÅ")
                    st.write(f"{emoji} {folder}: {count} files")
            else:
                st.info("No files found in uploads/")


def render_faust_docs_section(glm_system):
    """Render FAUST documentation section"""
    st.subheader("üéµ FAUST Documentation")
    if st.button("üì• Load FAUST Docs"):
        with st.spinner("Loading FAUST documentation..."):
            result = glm_system.file_processor.load_faust_documentation()
        st.success(result)

    if st.button("üåê Download FAUST Docs"):
        st.info("Run: python download_faust_docs_complete.py in your project directory")


def render_model_status(glm_system):
    """Render model status section with HRM integration"""
    st.subheader("üîß System Status")
    
    # HRM Status Section
    st.write("**üß† HRM Integration:**")
    hrm_wrapper = getattr(glm_system, 'hrm_wrapper', None)
    if hrm_wrapper:
        # Display HRM device status
        device = getattr(hrm_wrapper, 'device', 'unknown')
        if device == 'mps':
            st.success("‚ö° M4 Max MPS acceleration active")
        elif device == 'cuda':
            st.success("üöÄ CUDA acceleration active")
        elif device == 'cpu':
            st.info("üíª CPU processing mode")
        else:
            st.warning(f"‚ùì Unknown device: {device}")
        
        # Display HRM availability
        try:
            # Check if HRM can decompose tasks
            test_decomp = hrm_wrapper.decompose_task("test", context={})
            if hasattr(test_decomp, 'subtasks'):
                st.success("‚úÖ HRM task decomposition ready")
            else:
                st.warning("‚ö†Ô∏è HRM using pattern-based fallback")
        except Exception:
            st.warning("‚ö†Ô∏è HRM using pattern-based fallback")
            
        # HRM configuration info
        with st.expander("üîç HRM Configuration", expanded=False):
            st.write("**Architecture:** HRM ACT v1")
            st.write("**Device:** " + device.upper())
            st.write("**Caching:** Enabled" if getattr(hrm_wrapper, 'enable_caching', False) else "Disabled")
    else:
        st.error("‚ùå HRM not initialized")
    
    st.write("---")
    
    # Ollama Model Status
    st.write("**ü§ñ Ollama Models:**")
    for model_name, model_id in glm_system.models.items():
        try:
            if model_name in glm_system._model_instances:
                st.success(f"‚úÖ {model_name}")
            else:
                st.info(f"üí§ {model_name} (not loaded)")
        except:
            st.error(f"‚ùå {model_name}")

    if st.button("üîç Check Model Availability"):
        status = glm_system.check_model_availability()
        for model_name, status_text in status.items():
            if "‚úÖ" in status_text:
                st.success(f"{status_text} {model_name}")
            elif "‚ùå" in status_text:
                st.error(f"{status_text} {model_name}")
                st.code(f"ollama pull {glm_system.models[model_name]}")
            else:
                st.warning(f"{status_text} {model_name}")
    
    # Knowledge Base Status
    st.write("---")
    st.write("**üìö Knowledge Base:**")
    kb_status = glm_system.check_vectorstore_status()
    if kb_status["status"] == "‚úÖ Ready":
        st.success(f"‚úÖ {kb_status['document_count']} documents loaded")
    else:
        st.warning(kb_status["message"])


def render_chat_interface(glm_system, selected_model, use_context, selected_project, routing_mode="manual", debug_hrm=False):
    """Render main chat interface with hybrid routing support"""
    if selected_model == "auto":
        st.header(f"üí¨ Chat with AI Assistant (Auto-Routing)")
    else:
        st.header(f"üí¨ Chat with {selected_model}")
    st.caption(f"Project: {selected_project} | Mode: {routing_mode.title()}")

    # Project-based chat history
    chat_key = f"chat_history_{selected_model}_{selected_project}"
    if chat_key not in st.session_state:
        st.session_state[chat_key] = glm_system.project_manager.load_project_chats(
            selected_project, selected_model
        )

    # 1. CHAT INPUT FIRST (at the top)
    render_chat_input(
        glm_system, selected_model, use_context, selected_project, chat_key, routing_mode, debug_hrm
    )

    # Add some spacing
    st.write("---")

    # 2. RECENT CONVERSATIONS (below input)
    render_recent_conversations(st.session_state[chat_key], selected_model)

    # 3. FULL CHAT HISTORY (at the bottom)
    render_full_chat_history(st.session_state[chat_key], selected_model)

    # 4. Quick actions for FAUST (at the bottom)
    if selected_model == "auto" or "FAUST" in selected_model or "Code Llama" in selected_model:
        render_faust_quick_actions(
            glm_system, selected_model, use_context, selected_project, chat_key, routing_mode, debug_hrm
        )


def render_chat_input(
    glm_system, selected_model, use_context, selected_project, chat_key, routing_mode="manual", debug_hrm=False
):
    """Render chat input section with enhanced context handling"""
    # Add styling for the input section
    st.markdown(
        """
    <style>
    .chat-input-section {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 15px;
        border: 2px solid #2196F3;
        margin-bottom: 30px;
    }
    </style>
    """,
        unsafe_allow_html=True,
    )

    with st.container():
        st.markdown('<div class="chat-input-section">', unsafe_allow_html=True)
        st.subheader("üí¨ Ask Your Question")

        # Show context status
        col1, col2, col3 = st.columns(3)

        with col1:
            # Knowledge base status
            kb_status = glm_system.check_vectorstore_status()
            if kb_status["status"] == "‚úÖ Ready":
                st.success(f"üìö KB: {kb_status['document_count']} docs")
            elif kb_status["status"] == "‚ö†Ô∏è Empty":
                st.warning("üìö KB: Empty")
            else:
                st.error("üìö KB: Error")

        with col2:
            # Chat history status
            current_history = st.session_state.get(chat_key, [])
            if len(current_history) > 0:
                st.info(f"üí¨ History: {len(current_history)} exchanges")
            else:
                st.info("üí¨ History: New conversation")

        with col3:
            # Context toggle status
            if use_context:
                st.success("üß† Context: ON")
            else:
                st.warning("üß† Context: OFF")

        # Handle clear input flag
        default_value = ""
        if st.session_state.get("clear_chat_input", False):
            st.session_state.clear_chat_input = False  # Reset flag
            default_value = ""
        else:
            default_value = st.session_state.get("main_chat_input", "")

        question = st.text_area(
            "Type your question or request:",
            value=default_value,
            placeholder="""Examples:
        - Create a reverb effect in FAUST
        - Explain the uploaded C++ code
        - Design a low-pass filter
        - Help me debug this Python script
        - Continue our discussion about [previous topic]

        Press Enter for new lines, use the Send button when ready.""",
            key="main_chat_input",
            height=120,
            help="Multi-line input supported - great for code snippets and detailed questions!",
        )

        col1, col2, col3 = st.columns(3)

        with col1:
            send_button = st.button(
                "üöÄ Send", type="primary", disabled=not question.strip()
            )

        with col2:
            if st.button("üóëÔ∏è Clear Input"):
                st.session_state.clear_chat_input = True
                st.rerun()

        with col3:
            if question.strip():
                st.success(f"‚úÖ Ready to send ({len(question)} characters)")
            else:
                st.info("üí° Type your question above")

        # Additional controls
        col1, col2 = st.columns(2)

        with col1:
            if st.button(
                "üîÑ New Conversation", help="Start fresh without clearing history"
            ):
                st.session_state.main_chat_input = ""
                st.success("Ready for a new conversation!")

        with col2:
            if st.button("üóëÔ∏è Clear Chat History"):
                st.session_state[chat_key] = []
                st.success("Chat history cleared!")
                st.rerun()

        st.markdown("</div>", unsafe_allow_html=True)

        # Handle send button - ENHANCED WITH CHAT HISTORY
        if send_button and question:
            # Get current chat history
            current_history = st.session_state.get(chat_key, [])

            with st.spinner(f"{selected_model} is thinking..."):
                # Show what context is being used
                with st.expander("üîç Context Details", expanded=False):
                    st.write(f"**Using Context:** {'Yes' if use_context else 'No'}")
                    st.write(
                        f"**Chat History Length:** {len(current_history)} exchanges"
                    )

                    if use_context:
                        kb_status = glm_system.check_vectorstore_status()
                        st.write(f"**Knowledge Base:** {kb_status['message']}")

                        if len(current_history) > 0:
                            st.write("**Recent Topics:**")
                            for i, (prev_q, _) in enumerate(current_history[-3:], 1):
                                st.write(
                                    f"  {i}. {prev_q[:80]}{'...' if len(prev_q) > 80 else ''}"
                                )

                # Call model with hybrid routing system
                response_data = glm_system.generate_response(
                    prompt=question,
                    selected_model=selected_model,
                    routing_mode=routing_mode,
                    use_context=use_context,
                    project_name=selected_project,
                    chat_history=current_history,  # PASS CHAT HISTORY
                    use_hrm_decomposition=True
                )
                
                response = response_data["response"]
                routing_info = response_data.get("routing", {})

            # Add to chat history
            st.session_state[chat_key].append((question, response))
            
            # Display routing decision if available
            if routing_info:
                with st.expander("üîÑ Routing Decision", expanded=debug_hrm):
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Complexity", f"{routing_info.get('complexity_score', 0):.2f}")
                    with col2:
                        st.metric("Domain", routing_info.get('domain', 'general'))
                    with col3:
                        st.metric("Model Used", routing_info.get('selected_model', 'Unknown'))
                    
                    if routing_info.get('hrm_recommendation') != routing_info.get('selected_model'):
                        st.info(f"üí° HRM recommended: {routing_info.get('hrm_recommendation')}")
                    
                    if routing_info.get('subtasks', 0) > 0:
                        st.info(f"üß† Complex task decomposed into {routing_info.get('subtasks')} subtasks")
                        
                        # Show HRM debug details if enabled
                        if debug_hrm and routing_info.get('subtasks', 0) > 0:
                            st.write("**üîç HRM Debug Information:**")
                            st.json({
                                "execution_time": routing_info.get('execution_time', 0),
                                "confidence": routing_info.get('confidence', 0),
                                "routing_reason": routing_info.get('routing_reason', ''),
                                "mode": routing_info.get('mode', ''),
                            })
                    
                    if routing_info.get('fallback_used'):
                        st.warning(f"‚ö†Ô∏è Fallback used: {routing_info.get('routing_reason')}")
                        
                    # HRM device status in debug mode
                    if debug_hrm:
                        hrm_wrapper = getattr(glm_system, 'hrm_wrapper', None)
                        if hrm_wrapper:
                            device = getattr(hrm_wrapper, 'device', 'unknown')
                            st.write(f"**üñ•Ô∏è HRM Device:** {device.upper()}")
                            if device == 'mps':
                                st.success("‚ö° M4 Max MPS acceleration used")

            # Save to project file  
            actual_model_used = routing_info.get('selected_model', selected_model) if routing_info else selected_model
            glm_system.project_manager.save_chat_to_project(
                selected_project, actual_model_used, question, response
            )

            st.rerun()


def render_recent_conversations(chat_history, selected_model):
    """Render recent conversations section"""
    # Display recent chat (last 5 exchanges) - collapsible
    recent_chats = chat_history[-5:]  # Show last 5 exchanges

    if recent_chats:
        st.subheader("üí¨ Recent Conversations")

        for i, (question, answer) in enumerate(reversed(recent_chats), 1):
            # Create preview text for collapsed state
            question_preview = question.replace("\n", " ")[:80] + (
                "..." if len(question) > 80 else ""
            )

            # Detect content type for better icons
            if "```" in answer:
                content_icon = "üíª"  # Code
            elif any(
                keyword in answer.lower()
                for keyword in ["faust", "dsp", "audio", "signal"]
            ):
                content_icon = "üéµ"  # Audio/DSP
            elif len(answer) > 500:
                content_icon = "üìÑ"  # Long text
            else:
                content_icon = "üí°"  # General

            # Create expandable container for each chat
            with st.expander(
                f"{content_icon} Q{len(recent_chats) - i + 1}: {question_preview}",
                expanded=i <= 2,  # Keep last 2 exchanges expanded by default
            ):
                # Question section
                st.markdown("**üôã Your Question:**")
                st.code(question, language=None)

                # Answer section
                st.markdown(f"**ü§ñ {selected_model}:**")
                if "```" in answer:
                    st.markdown(answer)
                else:
                    st.write(answer)

                # Add metadata footer with actions
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.caption(f"üìù Q: {len(question)} chars")
                with col2:
                    st.caption(f"üìã A: {len(answer)} chars")
                with col3:
                    # Detect content type
                    if "```" in answer:
                        st.caption("üíª Code")
                    elif any(
                        keyword in answer.lower()
                        for keyword in ["faust", "dsp", "audio"]
                    ):
                        st.caption("üéµ Audio/DSP")
                    elif "http" in answer:
                        st.caption("üîó Links")
                    else:
                        st.caption("üí¨ Text")
                with col4:
                    # Quick copy button for code blocks
                    if "```" in answer:
                        if st.button(
                            "üìã Copy",
                            key=f"copy_recent_{len(recent_chats) - i + 1}",
                            help="Copy code to clipboard",
                        ):
                            # Note: Actual clipboard functionality would need additional setup
                            st.success(
                                "Code copied to clipboard! (feature coming soon)"
                            )

            # Add spacing between exchanges
            if i < len(recent_chats):
                st.write("")
    else:
        st.info(
            "üí° No recent conversations. Start chatting to see your exchanges here!"
        )


def render_full_chat_history(chat_history, selected_model):
    """Render full chat history section"""
    if chat_history:
        total_messages = len(chat_history)
        with st.expander(
            f"üìú Complete Chat History ({total_messages} conversations)",
            expanded=False,
        ):
            # Show last 10 conversations in reverse order (newest first)
            for i, (question, answer) in enumerate(reversed(chat_history[-10:]), 1):
                question_preview = question.replace("\n", " ")[:100] + (
                    "..." if len(question) > 100 else ""
                )

                with st.expander(
                    f"üîπ Conversation {total_messages - i + 1}: {question_preview}",
                    expanded=False,
                ):
                    col1, col2 = st.columns([1, 3])

                    with col1:
                        st.write("**Question:**")
                    with col2:
                        st.code(question, language=None)

                    col1, col2 = st.columns([1, 3])
                    with col1:
                        st.write("**Answer:**")
                    with col2:
                        if "```" in answer:
                            st.markdown(answer)
                        else:
                            st.write(answer)

                    # Show token/character count
                    st.caption(f"üìä Q: {len(question)} chars | A: {len(answer)} chars")

                if i < 10 and i < total_messages:  # Don't add divider after last item
                    st.write("---")


def render_faust_quick_actions(
    glm_system, selected_model, use_context, selected_project, chat_key, routing_mode="manual", debug_hrm=False
):
    """Render FAUST quick action buttons"""
    # Add styling for FAUST section
    st.markdown(
        """
    <style>
    .faust-actions {
        background-color: #f0f8ff;
        padding: 15px;
        border-radius: 10px;
        border: 2px solid #9C27B0;
        margin-top: 30px;
    }
    </style>
    """,
        unsafe_allow_html=True,
    )

    with st.container():
        st.markdown('<div class="faust-actions">', unsafe_allow_html=True)
        st.subheader("üéµ FAUST Quick Actions")
        col1, col2, col3 = st.columns(3)

        faust_actions = [
            ("üîä Basic Oscillator", FAUST_QUICK_PROMPTS["basic_oscillator"]),
            ("üéõÔ∏è Filter Design", FAUST_QUICK_PROMPTS["filter_design"]),
            ("üéØ Effect Chain", FAUST_QUICK_PROMPTS["effect_chain"]),
        ]

        for i, (button_text, question) in enumerate(faust_actions):
            with [col1, col2, col3][i]:
                if st.button(button_text):
                    with st.spinner("Generating code..."):
                        # Use hybrid routing for FAUST actions - favor Code Llama for FAUST tasks
                        response_data = glm_system.generate_response(
                            prompt=question,
                            selected_model="Code Llama (FAUST Specialist)" if routing_mode == "auto" else selected_model,
                            routing_mode="auto",  # Always use auto for FAUST quick actions
                            use_context=use_context,
                            project_name=selected_project,
                            chat_history=st.session_state.get(chat_key, []),
                            use_hrm_decomposition=True
                        )
                        
                        response = response_data["response"]
                        routing_info = response_data.get("routing", {})

                    st.session_state[chat_key].append((question, response))
                    
                    # Show FAUST-specific routing info
                    if routing_info.get('domain') == 'faust_synthesis':
                        st.success(f"üéµ FAUST specialist routing: {routing_info.get('selected_model')}")
                    
                    actual_model_used = routing_info.get('selected_model', selected_model) if routing_info else selected_model
                    glm_system.project_manager.save_chat_to_project(
                        selected_project, actual_model_used, question, response
                    )
                    st.rerun()

        st.markdown("</div>", unsafe_allow_html=True)
